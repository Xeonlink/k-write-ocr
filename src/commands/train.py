from typing import Annotated

import numpy as np
import typer
from rich.panel import Panel
from rich.progress import track
from rich.table import Table, box

from codec import KoreanCodec
from common import nn
from common.optimizer import Adam
from constants import DATA_DIR, MAX_LABEL_LENGTH, UINT8_MAX, console
from data_loader import LanguageDataLoader
from net import KOCRNet

app = typer.Typer(
    # name="train",
    # help="í•™ìŠµ ê³¼ì •ì„ ê´€ë¦¬ \n(í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë“±)",
    rich_markup_mode="rich",
)


@app.command("train", help="í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
def train(
    yes: Annotated[bool, typer.Option(help="í™•ì¸ì—†ì´ ì§„í–‰")] = False,
    max_epoch: Annotated[int, typer.Option(help="í•™ìŠµ ì—í­ ìˆ˜")] = 10,
    batch_size: Annotated[int, typer.Option(help="ë°°ì¹˜ í¬ê¸°")] = 32,
    verbose: Annotated[bool, typer.Option(help="ìƒì„¸ ë¡œê¹… ì—¬ë¶€")] = False,
) -> None:
    # ì‘ì—… ì„¤ëª… ì¶œë ¥í•˜ê¸°
    if not yes:
        panel_content = "\n".join(
            [
                f"ğŸ“‚ í•™ìŠµ ê³¼ì • í´ë”ì˜ ê²½ë¡œ",
                f"[blue]./{DATA_DIR}[/]",
            ]
        )
        panel = Panel(
            panel_content,
            title="Train",
            title_align="left",
            border_style="green bold",
            padding=(1, 2),
        )
        console.print(panel)

        if not typer.confirm(f"ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            console.print("Operation cancelled.", style="red")
            return

    # í•™ìŠµ ì‹œì‘
    codec = KoreanCodec(max_length=MAX_LABEL_LENGTH)
    train_loader = LanguageDataLoader(DATA_DIR / "train_labels.csv", codec, batch_size)
    test_loader = LanguageDataLoader(DATA_DIR / "test_labels.csv", codec, batch_size)

    loss_function = nn.MSELoss()
    optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)
    model = KOCRNet()

    for epoch in range(max_epoch):

        train_losses = []
        iter = 0
        for iter, (x, t) in enumerate(train_loader):
            # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
            x = x.astype(np.float32) / UINT8_MAX
            t = t.astype(np.float32) / UINT8_MAX

            # ëª¨ë¸ í•™ìŠµ
            pred = model.forward(x)  # foward ê°’ ê³„ì‚° (gradient ê³„ì‚° ë•Œ ì‚¬ìš©)
            loss = loss_function.forward(pred, t)  # ì†ì‹¤ ê³„ì‚°
            dout = loss_function.backward()  # ì†ì‹¤ í•¨ìˆ˜ ë¯¸ë¶„
            model.backward(dout)  # gradient ê³„ì‚°
            grads = model.gradient()  # gradient ê°’ ì¶”ì¶œ
            optimizer.update(model.params, grads)  # íŒŒë¼ë¯¸í„° update
            train_losses.append(loss)
            console.print(f"Epoch: {epoch+1}, Iter: {iter+1}, Loss: {loss}")

        total_count = len(test_loader)
        correct_count = 0
        for x, t in track(test_loader, description="Testing..."):
            # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
            x = x.astype(np.float32) / UINT8_MAX
            t = t.astype(np.float32) / UINT8_MAX

            # ëª¨ë¸ í‰ê°€
            pred = model.forward(x)
            decoded_pred = [codec.decode(pred[i]) for i in pred]
            decoded_t = [codec.decode(t[i]) for i in t]
            correct_count += sum(1 for pred, true in zip(decoded_pred, decoded_t) if pred == true)

        console.print(f"Accuracy: {correct_count / total_count * 100:03f}")
